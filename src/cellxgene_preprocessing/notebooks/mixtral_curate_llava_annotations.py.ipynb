{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f3a17-fe28-48f2-a057-0ae390d652bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "llava_annotations=pd.read_csv(snakemake.input.cellwhisperer_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073b30a-2d8e-4813-b269-abeb0965c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snakemake.input.model = '/msc/home/mschae83/text-generation-webui/models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52977fd-83e4-4be2-8af4-bed91eb6482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import (\n",
    "    Llama,\n",
    "    LlamaGrammar,\n",
    ")\n",
    "\n",
    "# load the model\n",
    "llm = Llama(\n",
    "    model_path=snakemake.input.model,\n",
    "    n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require more resources\n",
    "    n_threads=snakemake.threads,  # The number of CPU threads to use\n",
    "    n_threads_batch=snakemake.threads,\n",
    "    n_gpu_layers=40,  # 1 layers corresponds to  ~1GB VRAM \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11921bb-047e-4aa9-9195-9e4835186903",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5d6b1-6246-4f77-a87e-dd85efd26b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for idx, row in llava_annotations.iterrows():\n",
    "    if not isinstance(row[\"cluster_annotations\"], str):\n",
    "        response = \"No label\"\n",
    "    else:\n",
    "        output = llm(\n",
    "            f\"[INST] {snakemake.params.request}\\n{row['cluster_annotations']} [/INST]\",\n",
    "            max_tokens=1024,  # for training, we only use a max of 128. observe whether this matches..\n",
    "            stop=[\"</s>\"],  # stop token for Mixtral\n",
    "            logit_bias={\n",
    "                llm.tokenizer().encode(\"\\n\")[-1]: float(\"-inf\"),\n",
    "                llm.tokenizer().encode('\"')[-1]: float(\"-inf\")\n",
    "            },  # Prevent newlines and quotes\n",
    "            echo=False,  # don't echo the prompt as part of the response\n",
    "            seed=42,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "        )\n",
    "        response = output[\"choices\"][0][\"text\"].strip()\n",
    "    print(response)\n",
    "    results[idx] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6607b6dd-fa1c-435d-ac93-de26c1b3889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_annotations[\"curated_labels\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e56ba-6237-4fda-9b0a-581dd650bb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_annotations.to_csv(snakemake.output.curated_labels, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
