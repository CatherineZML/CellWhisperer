"""
CellWhisperer LLM validations based on

- perplexity

Next better evaluations could include

- how many of the top 100 genes can it recover?
- how well does it reproduce pathways?
"""

import pandas as pd


scattergather:  # same as in llava pipeline
    split=128

# This list is generated by the llava pipeline and, for convenience, tracked as part of the source code
ARCHS4_GSVA_SAMPLES = pd.read_csv(PROJECT_DIR / "src/llava/gsva_samples.csv", header=None).iloc[:, 0]

NUM_COMPLEX_SAMPLES = 5000
NUM_DETAILED_SAMPLES = 10000
CONVERSATION_START = NUM_COMPLEX_SAMPLES + NUM_DETAILED_SAMPLES
COMPLEX_SAMPLES = ARCHS4_GSVA_SAMPLES.to_list()[:NUM_COMPLEX_SAMPLES]
DETAILED_SAMPLES = ARCHS4_GSVA_SAMPLES.to_list()[NUM_COMPLEX_SAMPLES:CONVERSATION_START]

GPU_TYPE = "a100"


rule llava_evaluation_perplexity:
    """
    Current limitations:
    - Each of our datasets can only provide one evaluation dataset
    """
    input:
        llava_model=ancient(PROJECT_DIR / config["paths"]["llava"]["finetuned_model_dir"]),
        evaluation_dataset=PROJECT_DIR / config["paths"]["llava"]["evaluation_text_dataset"],
        # image_data=rules.process_full_dataset.output.model_outputs.format(dataset="{dataset}", model=config["model_name_path_map"]["cellwhisperer"]),
        image_data=rules.combine_processed_data.output.combined.format(model=config["model_name_path_map"]["cellwhisperer"]),
    conda:
        "llava"
    output:
        all_perplexities=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "all_perplexities.csv",
    params:
        num_projector_tokens=int(config["llava_projector_type"].split("_")[1].strip("t")),
        background_shuffle=lambda wildcards: "transcriptome" if wildcards.dataset == "archs4_geo" else "llm-response",
        num_negatives=30
    resources:
        mem_mb=300000,
        slurm=f"cpus-per-task=40 gres=gpu:{GPU_TYPE}:1 qos={GPU_TYPE} partition=gpu"
    log:
        "logs/llava_evaluation_perplexity/{dataset}_{base_model}_{model}.log"
    threads: 16
    notebook:
        "../notebooks/llava_evaluation_perplexity.py.ipynb"


rule llava_evaluation_perplexity_plots:
    """
    """

    input:
        all_perplexities=rules.llava_evaluation_perplexity.output.all_perplexities,
        mpl_style=ancient(PROJECT_DIR / config["plot_style"])
    output:
        log_perplexity_ratio=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "log_mean_perplexity.ratio",  # smaller is better log(ppl_real/ppl_neg_control)
        comparison_plot=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "perplexity_quantile.svg",  # barplot
        detailed_plot=PROJECT_DIR / config["paths"]["llava"]["evaluation_results"] / "detailed.svg",  # barplot
    conda:
        "cellwhisperer"
    notebook:
        "../notebooks/llava_evaluation_perplexity_plots.py.ipynb"
