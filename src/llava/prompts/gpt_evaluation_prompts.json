{
    "default":  {"role": "Assistant", "prompt": "We would like to request your feedback on the performance of multiple AI assistants in response to a question indicated in the [Question] block. The question refers to the state of a cell via its transcriptome. A ground truth description of this transcriptome is indicated in the [Reference] block, which contains a succinct natural language summary of the biological state of the sample, a prioritized list of the most prominently expressed genes (with the highest expression levels listed first) and  a prioritized list of the most active pathways, processes and other ontological terms (with the most significant ones listed first).\nFirst comprehensively assess the responses of each of the assistants with a focus on their relevance, accuracy, conciseness and level of detail, always with respect to the ground truth reference information. Make sure to detect 'honest mistakes' (e.g. the correct detection of a treatment, but with a slightly different (but related) compound; or the detection of a wrong but similar cell type, etc.) and judge them more generously than a completely wrong answer. Also, don't penalize information that goes beyond the provided reference, as long as it does not contradict it.\nBased on this assessment, provide one overall score per assistant on a scale of 1 to 10, with 1 indicating the worst of the provided responses and 10 indicating the best one. Here is the JSON template for your response: {\"explanation\": <explanation:str>, \"assistant_1\": <1-10:int>, \"assistant_2\": <1-10:int>, \"assistant_3\": <1-10:int>, ...}"}
}
