{
    "default":  {"role": "Assistant", "prompt": "We would like to request your feedback on the performance of multiple AI assistants in response to a question indicated in the [Question] block. The question refers to the state of a cell via its transcriptome. A ground truth description of this transcriptome is indicated in the [Reference] block, which contains a succinct natural language summary of the biological state of the sample, a prioritized list of the most prominently expressed genes (with the highest expression levels listed first) and  a prioritized list of the most active pathways, processes and other ontological terms (with the most significant ones listed first).\nPlease rate the helpfulness, relevance, accuracy, level of details of the assistants' responses, with respect to the ground truth reference information. Do not penalize brevity/conciseness and provide the scores on a scale of 1 to 10, with 1 indicating the worst possible response and 10 indicating a perfect response.\nPlease provide your response starting with a concise explanation of your evaluation for all assistants, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. Then, provide scores for all assistants, respectively. Here is the JSON template for your response: {\"explanation\": <explanation:str>, \"scores\": [<1-10:int_score_for _assistant1>, <1-10:int_score_for _assistant2>, ...]}"}
}
