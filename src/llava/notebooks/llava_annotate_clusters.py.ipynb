{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c4a2e-cddd-4e41-a943-e5f4cf1bfe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start coding here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b598e-3440-47b8-b3cf-bfe08516fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaVA imports \n",
    "\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX,\n",
    "    DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN,\n",
    "    DEFAULT_IM_END_TOKEN,\n",
    "    IMAGE_PLACEHOLDER,\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    get_model_name_from_path,\n",
    "    KeywordsStoppingCriteria,\n",
    "    tokenizer_image_token\n",
    ")\n",
    "\n",
    "# Other imports\n",
    "\n",
    "import anndata\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a54f8-364b-4167-8571-39b5534f2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "adata = anndata.read_h5ad(snakemake.input.adata)\n",
    "adata.obs[\"cluster_label\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a20aa-15e7-4556-ac43-f426d4a8db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique leiden clusters\n",
    "leiden_clusters = adata.obs['leiden'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048ae7a-74a5-4ddd-847d-bd6e9501e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "disable_torch_init()  # TODO test loading with and without to see speed benefit. Preferably get rid of it. With considerable speed up, use in my own inference?\n",
    "\n",
    "model_name = get_model_name_from_path(snakemake.input.llava_model)\n",
    "\n",
    "# set `base_model` for projector-only loading\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    snakemake.input.llava_model, model_base=None, model_name=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e998c1d-e461-4bd8-824f-ca621aad3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _infer_conv_mode(model_name):\n",
    "    if \"llama-2\" in model_name.lower():\n",
    "        return \"llava_llama_2\"\n",
    "    elif \"v1\" in model_name.lower():\n",
    "        return \"llava_v1\"\n",
    "    elif \"mpt\" in model_name.lower():\n",
    "        return \"mpt\"\n",
    "    else:\n",
    "        return \"llava_v0\"\n",
    "\n",
    "\n",
    "def prepare_conv(qs):\n",
    "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
    "    if IMAGE_PLACEHOLDER in qs:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
    "        else:\n",
    "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
    "    else:\n",
    "        if model.config.mm_use_im_start_end:\n",
    "            qs = image_token_se + \"\\n\" + qs\n",
    "        else:\n",
    "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs  # NOTE this one is being executed\n",
    "\n",
    "    conv_mode = _infer_conv_mode(model_name)\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv\n",
    "\n",
    "\n",
    "conv = prepare_conv(\"What does the sample describe?\")\n",
    "prompt = conv.get_prompt()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e6064-df53-4cd8-ae75-690d581e61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_for_cluster(cluster):\n",
    "    \n",
    "    # Clustering and mean over embeddings (alternative: mean over expression)\n",
    "    mean_cluster_embedding = adata.X[adata.obs['leiden'] == cluster].mean(axis=0)\n",
    "    \n",
    "    # TODO is float16 correct?\n",
    "    transcriptomes_tensor = torch.from_numpy(mean_cluster_embedding).to(\n",
    "        model.device, dtype=torch.float16\n",
    "    )\n",
    "    # Prepare model inputs and inference hyperparameters\n",
    "    input_ids = (\n",
    "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
    "        .unsqueeze(0)\n",
    "        .cuda()\n",
    "    )\n",
    "\n",
    "    return input_ids, transcriptomes_tensor\n",
    "input_ids, transcriptomes_tensor = features_for_cluster(leiden_clusters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273b9fa-fb75-4152-a13c-361998667321",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0\n",
    "num_beams = snakemake.params.num_beams\n",
    "top_p = 1.0\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "keywords = [stop_str]\n",
    "\n",
    "def predict(input_ids, transcriptomes_tensor):\n",
    "    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=transcriptomes_tensor.unsqueeze(0),\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            num_beams=num_beams,\n",
    "            max_new_tokens=200,\n",
    "            use_cache=True,\n",
    "            stopping_criteria=[stopping_criteria],\n",
    "        )\n",
    "    \n",
    "    input_token_len = input_ids.shape[1]\n",
    "    n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n",
    "    if n_diff_input_output > 0:\n",
    "        print(\n",
    "            f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\n",
    "        )\n",
    "    outputs = tokenizer.batch_decode(\n",
    "        output_ids[:, input_token_len:], skip_special_tokens=True\n",
    "    )[0]\n",
    "    outputs = outputs.strip()\n",
    "    if outputs.endswith(stop_str):\n",
    "        outputs = outputs[: -len(stop_str)]\n",
    "    outputs = outputs.strip()\n",
    "    return outputs\n",
    "\n",
    "predict(input_ids, transcriptomes_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a215959e-c144-4623-8bd8-275f81ebde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each cluster\n",
    "for cluster in leiden_clusters:\n",
    "    input_ids, transcriptomes_tensor = features_for_cluster(cluster)\n",
    "    outputs = predict(input_ids, transcriptomes_tensor)\n",
    "    adata.obs.loc[adata.obs.leiden == cluster, \"cluster_label\"] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667c89b-d3e9-4f3f-b8cd-10d2f4d5d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(snakemake.output.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53564662-0f7d-497d-abb2-7b97c34144b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
