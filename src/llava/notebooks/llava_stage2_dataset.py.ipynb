{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d20d1-8a99-4f1f-9a90-7c9bd4345f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import (\n",
    "    Llama,\n",
    "    LlamaGrammar\n",
    ")  # same speed as llama_cpp_cuda, but with tensor cores\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb848551-c9df-4dc7-880a-b7dcf41afd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load few-shot messages\n",
    "with open(snakemake.input.few_shot_messages) as f:\n",
    "    few_shot_messages = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069db233-d67f-419b-9c87-d6a7ee64b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load system message (instruction)\n",
    "\n",
    "system_prompt = Path(snakemake.input.instruction).read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34708a89-f45a-46f5-8f58-6f8252cb4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this split\n",
    "with open(snakemake.input.json_split) as f:\n",
    "    samples_request_messages = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22175b-0bdc-4993-b1a5-58dbc51c0b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "llm = Llama(\n",
    "    model_path=snakemake.input.model,\n",
    "    n_ctx=32000,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "    n_threads=25,  # The number of CPU threads to use\n",
    "    n_gpu_layers=86,  # High enough number to load the full model\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f10fe3-f1fd-403e-865d-92e2b4cc4bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_to_prompt(messages):\n",
    "    \"\"\"\n",
    "     `llm.create_chat_completion` did not work unfortunately for some reason,\n",
    "     so I needed to aggregate the prompt for myself\n",
    "    \"\"\"\n",
    "    return \"<s>\" + \"\".join(\n",
    "        [\n",
    "            '[INST] ' + message['content'] + ' [/INST]' if message['role'] == \"user\" else message['content'] + \"</s>\"\n",
    "            for message in messages\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def llm_run(messages, num_retries=3):\n",
    "    seed = hash(messages[-1][\"content\"]) % 2**30\n",
    "    for i in range(num_retries):\n",
    "        output = llm( \n",
    "            messages_to_prompt(messages),\n",
    "            max_tokens=2048,\n",
    "            stop=[\"</s>\"],  # stop token for Mixtral\n",
    "            seed=seed + i,\n",
    "            grammar=LlamaGrammar.from_json_schema(\n",
    "                Path(snakemake.input.json_schema).read_text()\n",
    "            ),\n",
    "            temperature=snakemake.params.temperature + i * (1.0/num_retries),\n",
    "            top_p=0.9 + i * (0.1 / 3),\n",
    "            top_k=50 + i * (100//num_retries),\n",
    "        )\n",
    "    \n",
    "        try:\n",
    "            result_serial = output[\"choices\"][0][\"text\"].strip()\n",
    "            logging.debug(result_serial)\n",
    "            result = json.loads(result_serial)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to decode : {type(e)}:{e}, {output}. Retrying {i}\")\n",
    "    else:\n",
    "        logging.warning(\n",
    "            f\"Prompt template continuously failed: \\n{messages[-1]}\\n{output}\"\n",
    "        )\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7907756-e7e7-46dd-a29d-b18611f6d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load chunked\n",
    "responses = {}\n",
    "for sample_id, sample_request_message in samples_request_messages.items():\n",
    "    if False:  # model.startswith(\"gpt-4\"):\n",
    "        messages = [{\"role\": \"assistant\", \"content\": system_prompt}]\n",
    "    else:\n",
    "        messages = [{\"role\": \"user\", \"content\": system_prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": \"Understood. Please provide the sample information, so I can generate a corresponding conversation.\"}]\n",
    "\n",
    "    messages.extend(few_shot_messages)  # TODO \n",
    "    try:\n",
    "        sample_request_message[\"content\"] += snakemake.params.prompt_reminder\n",
    "    except AttributeError:\n",
    "        pass\n",
    "        \n",
    "    messages.append(sample_request_message)\n",
    "        \n",
    "    response = llm_run(messages)\n",
    "\n",
    "    responses[sample_id] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c74c3a-e43a-427e-933e-b72543d5b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(snakemake.output.generated_conversations, \"w\") as f:\n",
    "    json.dump(responses, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
